# Debug set to true in order to debug high-layer code.
# CFG Configuration
# https://wandb.ai/poolc/huggingface/runs/r0pyyxyg/overview?workspace=user-snoop2head
CFG:
  DEBUG: false
  train_batch_size: 64
  valid_batch_size: 128

  # Train configuration
  num_epochs: 1 # validation loss is increasing after 5 epochs
  num_checkpoints: 3
  max_token_length: 64
  stopwords: []
  learning_rate: 0.0005 # has to be set as float explicitly due to https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number
  weight_decay: 0.01 # https://paperswithcode.com/method/weight-decay
  adam_beta_1: 0.9
  adam_beta_2: 0.98
  epsilon: 0.000000001
  fp16: false
  gradient_accumulation_steps: 2
  save_steps: 150
  logging_steps: 150
  evaluation_strategy: "epoch"

  # Evaluation configuration
  inference_model_name: "QuoQA-NLP/KE-T5-En2Ko-Base"
  no_inference_sentences: 100
  num_beams: 5
  repetition_penalty: 1.3
  no_repeat_ngram_size: 3
  num_return_sequences: 1

  # Translation settings
  dset_name: "LeverageX/AIHUB-all-parallel-ko-en" # or LeverageX/AIHUB-socio-parallel-ko-en
  src_language: "en"
  tgt_language: "ko"
  model_name: "KETI-AIR/ke-t5-base"
  num_inference_sample: 5000
  dropout: 0.1

  # wandb settings
  entity_name: "quoqa-nlp"
  project_name: "EN-TO-KO-Translation"

  # root path
  ROOT_PATH: "."
  save_path: "./results"
